{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6zoRErN30rN"
   },
   "source": [
    "The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. The columns are as follows, their names are pretty self-explanatory:\n",
    "\n",
    "1) Median House Value: Median house value for households within a block (measured in US Dollars) [USD]\n",
    "\n",
    "2) Median Income: Median income for households within a block of houses (measured in tens of thousands of US Dollars) [10kUSD]\n",
    "\n",
    "3) Median Age: Median age of a house within a block; a lower number is a newer building [years]\n",
    "\n",
    "4) Total Rooms: Total number of rooms within a block\n",
    "\n",
    "5) Total Bedrooms: Total number of bedrooms within a block\n",
    "\n",
    "6) Population: Total number of people residing within a block\n",
    "\n",
    "7) Households: Total number of households, a group of people residing within a home unit, for a block\n",
    "\n",
    "8) Latitude: A measure of how far north a house is; a higher value is farther north [°]\n",
    "\n",
    "9) Longitude: A measure of how far west a house is; a higher value is farther west [°]\n",
    "\n",
    "10) Distance to coast: Distance to the nearest coast point [m]\n",
    "\n",
    "11) Distance to Los Angeles: Distance to the centre of Los Angeles [m]\n",
    "\n",
    "12) Distance to San Diego: Distance to the centre of San Diego [m]\n",
    "\n",
    "13) Distance to San Jose: Distance to the centre of San Jose [m]\n",
    "\n",
    "14) Distance to San Francisco: Distance to the centre of San Francisco [m]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvO3bNEoTV9y"
   },
   "source": [
    "***1 point: Provide a clear explanation of the problem, the key objectives and goals, and how Machine Learning techniques (clustering, classification, and prediction) will be beneficial.***\n",
    "\n",
    "In this dataset, we have data on the housing market in California. Variables convey information about the location of the of neighborhood, median housing price there, and some information on population density.\n",
    "\n",
    "Key objectives are 1) to learn how to predict median housing price using several known variables ; 2) study housing market and find interesting relations that can help in the prediction modelling\n",
    "\n",
    "Having this dataset and using ML techniques it is possible to\n",
    "\n",
    "1) Analyze different patterns in the housing market in California with the use of clusterization. It is possible to find relationships between social-economical, house-specific and geographical variables in the housing market and find different groups.  \n",
    "\n",
    "2) Using predictive models, find a way how to estimate housing price based on given parameters. Additionally, it is possible to examine significance of each variable in the prediction models, so we can estimate which factors influence the housing prices more.\n",
    "\n",
    "If we jump in the data, we see that all independent variables (so all variables in the dataset except for the median housing prices) are known from the beginning. That is because we can find data on population from several statistical bureaus and geographical data are very easily obtainable.\n",
    "\n",
    "As all independent variables are known, it is always easy to use trained models for estimation which can further be applied in the business context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhHmdeW_TXAK"
   },
   "source": [
    "***2 points: Research on novel approaches and related research works implemented in the context of your selected task. Identify key challenges and State-of-the-Art (SotA) techniques from the literature review.***\n",
    "- **Clustering for Market Segmentation**  \n",
    "    Research Paper: _\"Review of Clustering Methods Used in Data-Driven Housing Market Segmentation\"_  \n",
    "    This paper highlights the utility of clustering algorithms such as K-Means and DBSCAN for grouping houses with similar characteristics. It supports our use of clustering to uncover hidden patterns in the California housing market, focusing on price differences and location. These insights provide better understanding of housing market segments and their unique attributes.\n",
    "    Link to Research Paper:https://www.researchgate.net/publication/373743133_Review_of_Clustering_Methods_Used_in_Data-Driven_Housing_Market_Segmentation\n",
    "    \n",
    "- **Predictive Modeling for Price Estimation**  \n",
    "    Research Paper: _\"Predicting Property Prices with Machine Learning Algorithms\"_  \n",
    "    This study demonstrates the effectiveness of algorithms like Random Forest and Gradient Boosting in property price prediction. It underscores the trade-offs between achieving high accuracy and maintaining interpretability, guiding our objective to balance these aspects. Additionally, it frames our research around managing multicollinearity and capturing non-linear relationships in housing data.\n",
    "    Link to Research Paper:https://www.researchgate.net/publication/346308101_Predicting_property_prices_with_machine_learning_algorithms\n",
    "    \n",
    "- **Feature Engineering and Advanced Regression**  \n",
    "    Research Paper: _\"Advanced Machine Learning Techniques for Predictive Modeling of Property Prices\"_  \n",
    "    This research emphasizes the importance of incorporating advanced feature engineering and regression models to improve prediction accuracy. It identifies gaps in integrating location and macroeconomic variables, aligning with our plan to include these features. The findings motivate our exploration of how geographical and economic factors influence housing prices.\n",
    "    Link to Research Paper:https://www.researchgate.net/publication/380803669_Advanced_Machine_Learning_Techniques_for_Predictive_Modeling_of_Property_Prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Rf0TaxHTXOw"
   },
   "source": [
    "***1 point: Identify and explain the types of Machine Learning tasks that need to be applied (e.g., cleaning, data transformation, scaling, normalization etc.) and are relevant to the problem. Provide an initial high-level diagram of the architecture and workflow of your approach.***\n",
    "\n",
    "\n",
    "According to the dataset author, the dataset was already cleaned. However, some outliers could be dropped (for example, several extremely expensive neighborhoods such as Beverly Hills in LA, or vice versa ghetos in Skidrow district).\n",
    "\n",
    "\n",
    "Standard scaling should be applied for variables that will be used in clusterizations. It is necessary since clusterization algorithms cannot work properly without scaling as they use distances in their code.\n",
    "\n",
    "Additionally, standard scaling should be used for all variables in the regression problems. Reason for this is because it can reduce the time complexity and save some time.\n",
    "\n",
    "Our workflow:\n",
    "\n",
    "1) Analyse data using diagrams / descriptive statistics in order to get as much understanding from data as possible. Find relationships with different variables and explore their distributions.\n",
    "\n",
    "2) Clean data and prepare it for further analysis (clusterizations and regressions) if necessary.\n",
    "\n",
    "3) Use clusterization techniques to get more insight on data distributions.\n",
    "\n",
    "4) Prepare data for the predictive modeling (split on train and validation set). Complete feature selection, prepare first model and check their performance.\n",
    "\n",
    "5) Fine tune model using validation set, find optimal parameters for all chosen variables. Explore this models. Get results on the test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpGUSqgkPDCw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXpYYb9Z2p7F"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('California_Houses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "IvgjcLbC4DjN",
    "outputId": "ff02123c-427e-45d2-a6e9-cec4a2081bf5"
   },
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbIUlR3CRJBf"
   },
   "source": [
    "# Part 1: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztCVhIKvXB0z"
   },
   "source": [
    "## Part 1.1: Potential challanges:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhrXbWlGXJ8f"
   },
   "source": [
    "Correlated variables, which can lead to multicollinearity.\n",
    "\n",
    "Continuously distributed data, which can lead to problems with clusterizations.\n",
    "\n",
    "Distance to the city is noisy. Using only 3 distances, we can get the coordinate of this district, and therefore getting the 4th distance. At the same time, Longtitude and Lattitude gives us the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "fGQ4zMZw4IDA",
    "outputId": "473a21e8-5e9f-40c9-d903-72d4bcf74241"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "vIIQMsJO4-cv",
    "outputId": "ada81177-9913-4adc-ace5-1f2edb91db7a"
   },
   "outputs": [],
   "source": [
    "data.isna().sum()\n",
    "\n",
    "#We have no missing values in this dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqPQc0lE5Dj8"
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "S0tqlX36Mq5e",
    "outputId": "4630e024-d918-4844-aefd-0ed526c12391"
   },
   "outputs": [],
   "source": [
    "plt.hist(data['Median_House_Value'], bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "9iyFRztqMUt9",
    "outputId": "c9cc1d14-bb2a-4d3c-be27-fa56e11d0883"
   },
   "outputs": [],
   "source": [
    "# Create a 2x\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].hist(data['Median_Income'],bins=10, color='blue')\n",
    "ax[0].set_title(\"Median Income\")\n",
    "\n",
    "ax[1].hist(data['Median_Age'],bins=10, color='green')\n",
    "ax[ 1].set_title(\"Median_Age\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "cASwI9xAO0dk",
    "outputId": "cbec19d5-8390-4b9e-e784-c6bfa8a9c8d0"
   },
   "outputs": [],
   "source": [
    "# Create a 2x2 grid of subplots\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8, 6))  # 2 rows, 2 columns\n",
    "\n",
    "# Plot each graph in its respective subplot\n",
    "ax[0, 0].hist(data['Tot_Rooms'],bins=15, color='blue')\n",
    "ax[0, 0].set_title(\"Total Rooms\")\n",
    "\n",
    "ax[0, 1].hist(data['Tot_Bedrooms'],bins=15, color='green')\n",
    "ax[0, 1].set_title(\"Total Bedrooms\")\n",
    "\n",
    "ax[1, 0].hist(data['Population'], bins=15, color='red')\n",
    "ax[1, 0].set_title(\"Population\")\n",
    "\n",
    "ax[1, 1].hist(data['Households'],bins=15, color='blue')\n",
    "ax[1, 1].set_title(\"Households\")\n",
    "\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#They are very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "mxTlD_u4O9JR",
    "outputId": "b2b17629-363c-4805-e522-8b1afba86257"
   },
   "outputs": [],
   "source": [
    "plt.hist(data['Distance_to_coast'], bins=15)\n",
    "plt.title('Distance to coast')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "UQvdkt1xQVYa",
    "outputId": "9c77abd7-9497-4870-8e26-2da286b3db76"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(8, 6))\n",
    "\n",
    "ax[0, 0].hist(data['Distance_to_LA'],bins=10, color='blue')\n",
    "ax[0, 0].set_title(\"Distance to LA\")\n",
    "\n",
    "ax[0, 1].hist(data['Distance_to_SanDiego'],bins=10, color='green')\n",
    "ax[0, 1].set_title(\"Distance to SanDiego\")\n",
    "\n",
    "ax[1, 0].hist(data['Distance_to_SanJose'], bins=10, color='red')\n",
    "ax[1, 0].set_title(\"Distance to SanJose\")\n",
    "\n",
    "ax[1, 1].hist(data['Distance_to_SanFrancisco'],bins=10, color='blue')\n",
    "ax[1, 1].set_title(\"Distance to SanFrancisco\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# From here we see problem with data - a lot of houses are in LA or close to it\n",
    "# But generally, we can see from here how many of them are in each city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6ptgh3zX_Ya"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "O_CR5YPJThBR",
    "outputId": "5ca353fc-8f1f-4648-9ce9-606a56f18933"
   },
   "outputs": [],
   "source": [
    "for name in data.columns[1:]:\n",
    "  sns.scatterplot(x=data[name].values, y=data['Median_House_Value'].values, color='blue', s=10)\n",
    "  plt.title(f\"Scatter Plot of {name} on Median House Value\")\n",
    "  plt.xlabel(f\"{name}\")\n",
    "  plt.ylabel(\"Median House Value\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XljXR6kUfXAf"
   },
   "source": [
    "From this set of scatterplots, we see that our dependent variable doesn't have exact linear relations with features, as well as any functional.\n",
    "\n",
    "We mostly see chaotic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "YuZRgYd4ThC8",
    "outputId": "bceac353-fba8-423d-a714-c93effb87837"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data['Longitude'].values, y=data['Latitude'].values, color='blue', s=10)\n",
    "plt.title(\"Comparison to the real California map\")\n",
    "plt.show()\n",
    "\n",
    "#This really looks like California\n",
    "#This map is done only to check how our data points are distributed on the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHaBN2b2xRgc"
   },
   "source": [
    "In all this plots, we see that there are outlier for the following variables:\n",
    "\n",
    "1) Total rooms (from the right side)\n",
    "\n",
    "2) Total Bedrooms (from the right side)\n",
    "\n",
    "3) Population\n",
    "\n",
    "4) Households\n",
    "\n",
    "5) Median Income\n",
    "\n",
    "6) Distance to coast\n",
    "\n",
    "7) Median_House_Value\n",
    "\n",
    "For this, we are going to omit this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAvlz5a1xxBt",
    "outputId": "4e0c98c2-b99d-446a-d91a-ba5b7a4d7d4b"
   },
   "outputs": [],
   "source": [
    "data.shape[0]\n",
    "#What is the size of dataset before dropping outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8OZC-RazX5H",
    "outputId": "38303db0-9508-4470-ccf5-1e7b4a8b8295"
   },
   "outputs": [],
   "source": [
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "outliers = iso.fit_predict(data)\n",
    "\n",
    "data_cleaned = data[outliers == 1]\n",
    "print(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOlDzQtExlR_",
    "outputId": "56b17e66-db4f-4993-f3aa-f72793a75c7c"
   },
   "outputs": [],
   "source": [
    "data_cleaned.shape[0]\n",
    "#How many are left after outlier cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "IWgg8XBn5xyc",
    "outputId": "1bee9a6f-cdb7-422d-d60d-01bfb491b8af"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(8, 6))  # 2 rows, 2 columns\n",
    "\n",
    "# Plot each graph in its respective subplot\n",
    "ax[0, 0].hist(data_cleaned['Tot_Rooms'],bins=15, color='blue')\n",
    "ax[0, 0].set_title(\"Total Rooms\")\n",
    "\n",
    "ax[0, 1].hist(data_cleaned['Tot_Bedrooms'],bins=15, color='green')\n",
    "ax[0, 1].set_title(\"Total Bedrooms\")\n",
    "\n",
    "ax[1, 0].hist(data_cleaned['Population'], bins=15, color='red')\n",
    "ax[1, 0].set_title(\"Population\")\n",
    "\n",
    "ax[1, 1].hist(data_cleaned['Households'],bins=15, color='blue')\n",
    "ax[1, 1].set_title(\"Households\")\n",
    "\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jGsqX_y6RaF"
   },
   "source": [
    "Now these variables look like having less outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjACBxXGztZA"
   },
   "source": [
    "We have a lot of data entries, therefore reducing them by 1032 is not problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ercHMxZs6R10"
   },
   "outputs": [],
   "source": [
    "data=data_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlGuYqASp8sT",
    "outputId": "5be885bb-1c24-459b-acc6-8426c9288946"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byY4AeM25HgP"
   },
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "id": "XyFggrdzQVfT",
    "outputId": "11a14bc9-7a7a-4f51-f9ab-a7c47faf2ad4"
   },
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm',annot_kws={'size': 7}, vmin=-1, vmax=1, square=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "VZ_K8E7UQVhy",
    "outputId": "83eca53c-695d-46bf-87e9-2f7e6354c98d"
   },
   "outputs": [],
   "source": [
    "print(data[['Tot_Bedrooms','Households','Population','Tot_Rooms']].corr())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(data[['Tot_Bedrooms','Households','Population','Tot_Rooms']].corr(),\n",
    "            annot=True, cmap='coolwarm',annot_kws={'size': 7}, vmin=-1, vmax=1, square=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjCUkaaYQ3Px"
   },
   "source": [
    "We have 3 blocks of information\n",
    "\n",
    "1) Income and Age - characteristics of neighborhood inhabitants and houses there.\n",
    "\n",
    "2) Population, Total Rooms and Bedrooms, Number of households - basically about the size/density of a neighborhood. We don't need all these columns for sure as they flow one from another\n",
    "\n",
    "3) Geographical: Distances to cities, Longtitude/Latitude. We need decided which will be omitted and which we need to take. It seems to be important as if district is close to those cities, prices should be higher. The same works with coast. But on the other hand, Longtitude and Latitude give the same info.\n",
    "\n",
    "Problems here:\n",
    "\n",
    "1) Multicollinearity.\n",
    "\n",
    "2) Low correlations between Mediah House Value and other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJxm2bkhRQG1"
   },
   "source": [
    "Ideas:\n",
    "\n",
    "1) Median Income: Higher Income - higher house value\n",
    "\n",
    "2) Population, Total Rooms, Total Bedrooms, Households: more of them - means its a block of flats district. House values lower.\n",
    "\n",
    "3) Distances: closer to cities - higher prices. LA the most expensive one, then it should have the most of expensive houses. The same works with distance to the coast. Houses that are closer to the coast, should be more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "kPHZmwigcq2F",
    "outputId": "89174668-0a1d-4c69-92c6-9a5b77cc8168"
   },
   "outputs": [],
   "source": [
    "X = data[['Latitude','Longitude']]\n",
    "scaler= StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "unique_labels = set(labels)\n",
    "for label in unique_labels:\n",
    "    label_mask = labels == label\n",
    "    plt.scatter(X_scaled[label_mask, 0], X_scaled[label_mask, 1], s=5, label=f'Cluster {label}')\n",
    "\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=50, c='black', marker='X', label='Centroids')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#This doesn't give us much of information, but generally shows us what is the distribution of districts in data across state and the centers\n",
    "#n=6 is chosen by 4 cities + rural areas of state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4kN16Z75Nla"
   },
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R6j_ka9-EY12",
    "outputId": "14884cb7-6c3c-4dbf-806a-1f4b0f9a0f84"
   },
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "min_samples = 5\n",
    "\n",
    "# Iterate over all pairs of features\n",
    "for feature_pair in combinations(data.columns, 2):\n",
    "    feature1, feature2 = feature_pair\n",
    "    X = data[[feature1, feature2]].values\n",
    "    scaler=StandardScaler()\n",
    "    X=scaler.fit_transform(X)\n",
    "\n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=10)\n",
    "    plt.title(f\"DBSCAN on {feature1} vs {feature2}\")\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print unique cluster labels\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    print(f\"Features: {feature1} & {feature2}\")\n",
    "    print(f\"Unique cluster labels: {unique_labels}\")\n",
    "    print(f\"Number of noise points: {n_noise}\\n\")\n",
    "\n",
    "\n",
    "    ### If you unhide output below, you can find 91 amazing scatterplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KlrKPo53Wc3i",
    "outputId": "307d7c9b-c461-4cd6-8a6c-708180e1b2aa"
   },
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "min_samples = 5\n",
    "\n",
    "for feature_triplet in combinations(data.columns, 3):\n",
    "    feature1, feature2, feature3 = feature_triplet\n",
    "    X = data[[feature1, feature2, feature3]].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "\n",
    "    # Plot the results\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels, cmap='viridis', s=10)\n",
    "    ax.set_title(f\"DBSCAN on {feature1}, {feature2}, {feature3}\")\n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2)\n",
    "    ax.set_zlabel(feature3)\n",
    "    plt.colorbar(scatter, ax=ax, label=\"Cluster Label\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print unique cluster labels\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    print(f\"Features: {feature1}, {feature2}, {feature3}\")\n",
    "    print(f\"Unique cluster labels: {unique_labels}\")\n",
    "    print(f\"Number of noise points: {n_noise}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kk0E3wAWyYiV"
   },
   "source": [
    "We liked combinations of Total Rooms, Bedrooms and Latitude from the scatterplots.\n",
    "\n",
    "We also liked combination of Total rooms on Distance to Coast.\n",
    "\n",
    "We implemented a search for parameters that will give the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "nVl_wYGjZr4N",
    "outputId": "a94466a8-8d61-4892-de2f-3f41c19dc797"
   },
   "outputs": [],
   "source": [
    "#our final\n",
    "X=data[[ 'Tot_Rooms','Tot_Bedrooms', 'Latitude' ]]\n",
    "scaler=StandardScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "\n",
    "epsilon_range = np.linspace(0.01, 0.4, 10)  # Adjust range and step size as needed\n",
    "min_samples_range = range(5, 16,2)\n",
    "\n",
    "# Variables to store optimal parameters and scores\n",
    "best_epsilon = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "\n",
    "# Grid search over epsilon and min_samples\n",
    "for epsilon in epsilon_range:\n",
    "    for min_samples in min_samples_range:\n",
    "        # Apply DBSCAN\n",
    "        model = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "        labels = model.fit_predict(X)\n",
    "\n",
    "        # Skip iterations where DBSCAN identifies no clusters\n",
    "        if len(set(labels)) <= 1:  # Only noise or single cluster\n",
    "            continue\n",
    "\n",
    "        # Calculate silhouette score (only works if more than 1 cluster exists)\n",
    "        score = silhouette_score(X, labels)\n",
    "\n",
    "        # Update best parameters if this combination is better\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_epsilon = epsilon\n",
    "            best_min_samples = min_samples\n",
    "\n",
    "# Output results\n",
    "print(f\"Optimal epsilon: {best_epsilon}\")\n",
    "print(f\"Optimal min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Optional: Visualize clusters for the optimal parameters\n",
    "model = DBSCAN(eps=best_epsilon, min_samples=best_min_samples)\n",
    "labels = model.fit_predict(X)\n",
    "\n",
    "# Plot the 3D clusters\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels, cmap='viridis', s=50)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"DBSCAN Clustering (Optimal Parameters)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "L9xWT1lgqpEG",
    "outputId": "fe28036f-75a8-4033-8341-d28d66013656"
   },
   "outputs": [],
   "source": [
    "model = DBSCAN(eps=0.4, min_samples=11)\n",
    "labels = model.fit_predict(X)\n",
    "\n",
    "# Plot the 3D clusters\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels, cmap='viridis', s=10)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"DBSCAN Clustering (Optimal Parameters)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "eI2Gi9Dsv6ya",
    "outputId": "1165ea51-6004-4670-92c2-3a5938c191be"
   },
   "outputs": [],
   "source": [
    "X=data[['Tot_Bedrooms','Distance_to_coast' ]]\n",
    "scaler=StandardScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "\n",
    "epsilon_range = np.linspace(0.05, 1.0, 10)  # Adjust range and step size as needed\n",
    "min_samples_range = range(3, 16,2)\n",
    "\n",
    "# Variables to store optimal parameters and scores\n",
    "best_epsilon = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "\n",
    "# Grid search over epsilon and min_samples\n",
    "for epsilon in epsilon_range:\n",
    "    for min_samples in min_samples_range:\n",
    "        # Apply DBSCAN\n",
    "        model = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "        labels = model.fit_predict(X)\n",
    "\n",
    "        # Skip iterations where DBSCAN identifies no clusters\n",
    "        if len(set(labels)) <= 1:  # Only noise or single cluster\n",
    "            continue\n",
    "\n",
    "        # Calculate silhouette score (only works if more than 1 cluster exists)\n",
    "        score = silhouette_score(X, labels)\n",
    "\n",
    "        # Update best parameters if this combination is better\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_epsilon = epsilon\n",
    "            best_min_samples = min_samples\n",
    "\n",
    "# Output results\n",
    "print(f\"Optimal epsilon: {best_epsilon}\")\n",
    "print(f\"Optimal min_samples: {best_min_samples}\")\n",
    "print(f\"Best silhouette score: {best_score}\")\n",
    "\n",
    "# Optional: Visualize clusters for the optimal parameters\n",
    "model = DBSCAN(eps=best_epsilon, min_samples=best_min_samples)\n",
    "labels = model.fit_predict(X)\n",
    "\n",
    "# Plot the 2D clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.colorbar(scatter, label='Cluster Label')\n",
    "plt.title(\"DBSCAN Clustering (Optimal Parameters)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBZd8Xj2yvdc"
   },
   "source": [
    "Here we see that these DBSCAN basically suggest that all points are from the same cluster.\n",
    "\n",
    "We are not getting any insights from them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AODnNwAORUKF"
   },
   "source": [
    "From Bin(14,2) + Bin(14,3) scatterplots and possible DBSCANs we can conclude that for this case DBSCAN is not the best algorithm for clusterization.\n",
    "\n",
    "There is a theoretical reason behind that - in the way how DBSCAN operates.\n",
    "\n",
    "\n",
    "While Kmeans just \"throws\" n points as centroids and then repeats this until algorithm converges, DBSCAN operates in a different way.\n",
    "\n",
    "DBSCAN marks point in 3 different classes: inside points, border points and noisy point. Each inside point should have at least M (min_sample parameter) within epsilon (eps parameter) distance. Border points lie within epsilon distance but have less than M points of this class. Noisy points don't respect these two conditions. So if there is, for example, a dataset which is on the scatterplot is just a straight line made out of many points, it will obviously be marked as a single cluster (while kmeans will set few different clusters).\n",
    "\n",
    "\n",
    "And after seeing scatterplots for all possible variables (in both 2D and 3D) we can see that all points are very dense and there is almost no way to find stable clusters.\n",
    "\n",
    "What could be done: using PCA new features can be created, but we decided not to go in that direction because with PCA the meaning of variables is getting lost, therefore none insights can be obtained. It will not increase our understanding of data and will not give us any ideas that can be later used in the predictive analysis.\n",
    "\n",
    "Also we could do it just based on locations, but it will be just a cartography in which we are not very interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ljk3HeVG5c66"
   },
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "QzZM3QfIeApl",
    "outputId": "7b92f5c2-8f29-4ee3-fedb-d27dfc9047dc"
   },
   "outputs": [],
   "source": [
    "#optimal clusters for k-means\n",
    "\n",
    "X = data[['Latitude','Longitude','Median_Income']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "cluster_range = range(1, 10)\n",
    "inertia_values = []\n",
    "\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(cluster_range, inertia_values, marker='o')\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Within-cluster Sum of Squares\")\n",
    "plt.show()\n",
    "\n",
    "#optimal clusters = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "_KkgMxxcadIG",
    "outputId": "2c8bf82e-85c0-44a2-952e-cd061a764d64"
   },
   "outputs": [],
   "source": [
    "#Kmeans clusterization + graph\n",
    "\n",
    "X = data[['Latitude','Longitude','Median_Income']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "kmeans.fit(X_scaled)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(X_scaled[:, 0], X_scaled[:, 1], X_scaled[:, 2],\n",
    "                     c=labels, cmap='viridis', s=10)\n",
    "ax.set_title(\"K-Means Clustering\")\n",
    "ax.set_xlabel(\"Latitude (scaled)\")\n",
    "ax.set_ylabel(\"Longitude (scaled)\")\n",
    "ax.set_zlabel(\"Median Income (scaled)\")\n",
    "\n",
    "legend = fig.colorbar(scatter, ax=ax, label='Cluster Label')\n",
    "plt.show()\n",
    "\n",
    "silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "wcss = kmeans.inertia_\n",
    "print(f\"Within-Cluster Variance (WCSS): {wcss:.2f}\")\n",
    "\n",
    "#This graph basically divides all state in few clusters (which are close to main cities) and shows income distribution\n",
    "# across different places\n",
    "# basically from here we know that in LA people are very rich\n",
    "#Silhouette Score: 0.41\n",
    "#Within-Cluster Variance (WCSS): 11838.89\n",
    "#This shows us that this clusterization is not that strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R67ErgvsRQJC"
   },
   "source": [
    "# Part 2: Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJoidMWtwlGe"
   },
   "source": [
    "## Model initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D47lVZWer418"
   },
   "outputs": [],
   "source": [
    "# This is all we have as a data pipeline:\n",
    "\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, contamination=0.05, random_state=42):\n",
    "        self.contamination = contamination\n",
    "        self.random_state = random_state\n",
    "        self.isolation_forest = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.isolation_forest = IsolationForest(\n",
    "            contamination=self.contamination, random_state=self.random_state\n",
    "        )\n",
    "        self.isolation_forest.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        outliers = self.isolation_forest.predict(X)  # -1 for outliers, 1 for inliers\n",
    "        return X[outliers == 1]\n",
    "\n",
    "# Example usage\n",
    "outlier_remover = OutlierRemover(contamination=0.05)\n",
    "data_cleaned = outlier_remover.fit_transform(data)\n",
    "data=data_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "id": "dNIg59SUjkjh",
    "outputId": "e8b03f27-d809-449b-9224-1481c9e72276"
   },
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm',annot_kws={'size': 7}, vmin=-1, vmax=1, square=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S01KW-CM7RrA"
   },
   "source": [
    "We want to analyze our dataset and make predictions using three models:\n",
    "\n",
    "1) Linear Model (including different variations); Reason for this choice is that this is the most used and the easiest model. In addition, we are in the social science / policy proposal setting, where linear regression are used the most.\n",
    "\n",
    "2) Decision Tree Regressor; Reason: Just why not - it is interesting to check its performance on the dataset like this one; In addition, it is a simple algorithm that does not have any problems because of the multicollinearity.\n",
    "\n",
    "3) XGBoost; Reason: this model is one of the most powerful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD59bhp07vVj"
   },
   "source": [
    "However, a problem arises because we are choosing a linear model. Take a look at our data.\n",
    "\n",
    "There is high correlation between several variables and therefore there is a possible multicolinearity.\n",
    "\n",
    "If from one side, it is not problematic for Ensemble or Tree models, it is a great issue for the linear model.\n",
    "\n",
    "It is problematic since with multicollinearity\n",
    "\n",
    "1) Our beta coefficients can be unstable;\n",
    "\n",
    "2) Our gradient descent algorithm which is incorporated into linear models could have issues with finding a min and therefore finding optimal betas.\n",
    "\n",
    "Therefore, we need to check for multicolinearity.\n",
    "\n",
    "1) If there is no multicollinearity - the best strategy then would be leaving all variables;\n",
    "\n",
    "2) If there is one - we need to drop some variables.\n",
    "\n",
    "The problem of this way is that we can loose some important information for Trees and XGBoost while gaining only in predictive power of linear model.\n",
    "\n",
    "So now we need to work with feature selection and explore for milticollinearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLkCLXJRRQNb"
   },
   "source": [
    "We see that there are two blocks of Highly-correlated data.\n",
    "\n",
    "1) On neighborhood parameters (Number of Rooms, Bedrooms, Population, Households). It is obvious why they are correlated. More people is almost the same with more household. More people $\\to$ more rooms needed (and bedrooms as well).\n",
    "\n",
    "2) Location parameters. Mathematically they are multicolinear. We have distances from different cities + coordinates. We are almost sure that they were obtained as just taking distance function, so all distances are coming from coordinates. Aditionally, if we have 3 distances from 3 different point in space, we can obtain its coordinate and therefore get 4th coordinate.\n",
    "\n",
    "However, there is a problem.\n",
    "\n",
    "1) With neighborhood parameters, omitting variables can lead to some problems with the predictive power of non-linear models.\n",
    "\n",
    "2) With coordinates, omitting one variable (let's say distance to SF) can lead to a lot of problems with predictive power. The reason for this is that housing prices are usually higher if house is close to the city centre + housing prices in cities are higher + housing prices vary across cities. So here we can lost a lot of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJeBr3kO_l-D"
   },
   "source": [
    "VIF Test can help us to explore whether there is a multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLssm5kxmgba",
    "outputId": "27630247-81f3-4dbe-a03f-b39aa636e608"
   },
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "X = data[['Distance_to_coast',\n",
    "               'Distance_to_LA' ,\n",
    "           'Distance_to_SanJose',\n",
    "          'Distance_to_SanFrancisco',\n",
    "          'Distance_to_SanDiego'\n",
    "          ]].copy()\n",
    "\n",
    "# Add a constant for intercept\n",
    "X_cs = add_constant(X)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_cs.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_cs.values, i) for i in range(X_cs.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-r8s9tJu3FTF",
    "outputId": "737bc72b-5212-4a55-f210-714eaa3f6c51"
   },
   "outputs": [],
   "source": [
    "X = data[['Distance_to_coast',\n",
    "               'Distance_to_LA' ,\n",
    "           'Distance_to_SanJose',\n",
    "          'Distance_to_SanDiego'\n",
    "          ]].copy()\n",
    "\n",
    "# Add a constant for intercept\n",
    "X_cs = add_constant(X)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_cs.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_cs.values, i) for i in range(X_cs.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pIW21sMpBKn",
    "outputId": "56b27c94-38a4-4c88-f956-7e9deef438a3"
   },
   "outputs": [],
   "source": [
    "X = data[['Distance_to_coast',\n",
    "               'Distance_to_LA' ,\n",
    "           'Distance_to_SanJose',\n",
    "          ]].copy()\n",
    "\n",
    "# Add a constant for intercept\n",
    "X_cs = add_constant(X)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_cs.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_cs.values, i) for i in range(X_cs.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4B4inDTozOb"
   },
   "source": [
    "VIF analysis shows that there is multicollinearity indeed. However, we tested this problem later with analysis later.\n",
    "\n",
    "Omitting two variables solves this issue indeed. In the next few code chunks we are going to test Linear Regression and XGBoost and check how it changes.\n",
    "\n",
    "Our candidates to be omitted: Distances to San Diego and San Francisco.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiFUm3G7_v2v"
   },
   "source": [
    "Now, let's check for multicollinearity in the neighborhood parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hA9XCjR_-af",
    "outputId": "bc54983f-367a-40ad-9b53-393be22adb16"
   },
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "X = data[['Tot_Rooms',\n",
    "               'Tot_Bedrooms' ,\n",
    "           'Population',\n",
    "          'Households'\n",
    "          ]].copy()\n",
    "\n",
    "# Add a constant for intercept\n",
    "X_cs = add_constant(X)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_cs.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_cs.values, i) for i in range(X_cs.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khUpKx9J_-eV",
    "outputId": "195afe34-9f9c-499f-d78e-e5204c307d67"
   },
   "outputs": [],
   "source": [
    "X = data[['Tot_Rooms',\n",
    "               'Tot_Bedrooms' ,\n",
    "           'Population',\n",
    "          ]].copy()\n",
    "\n",
    "# Add a constant for intercept\n",
    "X_cs = add_constant(X)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_cs.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_cs.values, i) for i in range(X_cs.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6Gwjd58AJw-"
   },
   "source": [
    "Test shows there is a multicollinearity as well.\n",
    "\n",
    "Deleting Household variable can solve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ABgtExejlBL"
   },
   "outputs": [],
   "source": [
    "# Here we will be initializing our data selection\n",
    "# We don't know yet which feautures we will select and therefore we are will come back to this block many time\n",
    "\n",
    "\n",
    "\n",
    "all_features_list=['Median_Income','Median_Age',\n",
    "               'Tot_Rooms','Tot_Bedrooms','Population','Households',\n",
    "               'Latitude','Longitude',\n",
    "               'Distance_to_coast',\n",
    "               'Distance_to_LA', 'Distance_to_SanDiego' , 'Distance_to_SanJose','Distance_to_SanFrancisco'] #only for ctlc+ctrlv\n",
    "\n",
    "features_list=['Median_Income','Median_Age',\n",
    "               'Tot_Rooms','Tot_Bedrooms','Population',\n",
    "               #'Households',\n",
    "               'Latitude','Longitude',\n",
    "               'Distance_to_coast',\n",
    "               'Distance_to_LA',\n",
    "              'Distance_to_SanJose'\n",
    "               ,'Distance_to_SanDiego','Distance_to_SanFrancisco'\n",
    "               ]\n",
    "\n",
    "distance_columns=['Distance_to_coast',\n",
    "               'Distance_to_LA', 'Distance_to_SanDiego' , 'Distance_to_SanJose','Distance_to_SanFrancisco']\n",
    "\n",
    "y=data['Median_House_Value']\n",
    "X=data[features_list]\n",
    "\n",
    "\n",
    " = train_test_split(X, y, test_size=0.3, random_state=40)\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1Fj5nrG11hh"
   },
   "source": [
    "Now we will write the simpliest models in functions and then check their performance.\n",
    "\n",
    "With the use of them, we will complete the feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUAh_hvLn6n0"
   },
   "outputs": [],
   "source": [
    "def LRegression(X_train,y_train,X_test,y_test):\n",
    "  model = LinearRegression()\n",
    "  model.fit(X_train, y_train)\n",
    "  y_test_pred = model.predict(X_test)\n",
    "  mae = mean_absolute_error(y_test, y_test_pred)\n",
    "  mse = mean_squared_error(y_test, y_test_pred)\n",
    "  print(\"Model R^2:\", model.score(X_test, y_test))\n",
    "  print(\"Model MAE:\", mae)\n",
    "  print(\"Model MSE:\", mse)\n",
    "\n",
    "def XGBoost(X_train,y_train,X_test,y_test):\n",
    "  model = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "  print('Mean Squared Error:', mse)\n",
    "  print('R^2 Score:', r2)\n",
    "\n",
    "def DecisionTreeModel(X_train, y_train, X_test, y_test):\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print('Test Mean Absolute Error:', mae)\n",
    "    print('Test Mean Squared Error:', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzTALE7suihU"
   },
   "source": [
    "Now we are going to test if omitting variables will increase our predictive power.\n",
    "\n",
    "We will be omitting Distance to SF and Distance to SD\n",
    "\n",
    "We will be comparing a simple linear model and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lNC2IsiCuuok",
    "outputId": "8acf807a-aa5d-4ea1-864f-3a8e808979ff"
   },
   "outputs": [],
   "source": [
    "#Linear regression with all variables\n",
    "LRegression(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AN3ajFSuura",
    "outputId": "cef13e01-d3ed-486e-f009-8a3a48bf1877"
   },
   "outputs": [],
   "source": [
    "#Linear regression without Distances to San Francisco and San Diego\n",
    "LRegression(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_RnE2_V_BGjT",
    "outputId": "ba3304e6-1d81-4a4e-9c9e-3129c5578d3d"
   },
   "outputs": [],
   "source": [
    "4621984396.3485565-4610948091.588843"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX5L-Uv-vk3N"
   },
   "source": [
    "Difference in models is positive, $R^2$ decreased, so it is even worse to build model without those variables.\n",
    "\n",
    "And what is more important, we got it on a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvUqEqisuuuV",
    "outputId": "a47ed22d-754a-4b5b-859e-f6f62a716640"
   },
   "outputs": [],
   "source": [
    "#XGBoost with all variables\n",
    "XGBoost(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wikNQENevF0H",
    "outputId": "e04dc449-a1d8-43c4-960e-edad26bfdf0c"
   },
   "outputs": [],
   "source": [
    "#XGBoost without Distances to San Francisco and San Diego\n",
    "XGBoost(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOIfcH2bvo04"
   },
   "source": [
    "MSE in both cases increased and $R^2$ decreased (not significantly, however). This means that distances to these cities could be explaining some of the variance and can be important for predictions in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NEuTDaYvOB4"
   },
   "source": [
    "Now let's remember that there is high correlation for rooms, bedrooms, population and households.\n",
    "\n",
    "Let's follow the same procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tUgTXBkq7av"
   },
   "source": [
    "Now let's come to Households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KC2wNQAvvZIT",
    "outputId": "2ac47613-f0fb-49f0-f309-51f95edf2798"
   },
   "outputs": [],
   "source": [
    "#Linear regression with all variables\n",
    "LRegression(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXsT_I8zvZje",
    "outputId": "0750b178-ccd7-4ae1-bd5d-7a4cf1e7bff7"
   },
   "outputs": [],
   "source": [
    "#Linear regression without households\n",
    "LRegression(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3TfK5nGrFhE",
    "outputId": "730169d1-b45f-4a11-ddc1-f688d57defdf"
   },
   "outputs": [],
   "source": [
    "#XGBoost with all variables\n",
    "XGBoost(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2tFQZNkrFki",
    "outputId": "d9ee77ad-3119-40d1-cefb-4069d905dda6"
   },
   "outputs": [],
   "source": [
    "#XGBoost without households\n",
    "XGBoost(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7LB0zJvrhaq"
   },
   "source": [
    "$R^2$ increased (MSE decreased) for the Linear model and vice versa for XGBoost, which is interesting. This is not due to possible multicollolinearity.\n",
    "\n",
    "Now let's perform optimal variable search and make a final decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irVhfNPmsKmQ"
   },
   "source": [
    "We are doing forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxNb9RLkrz4x",
    "outputId": "d6281be5-edda-4f66-8405-2d2ad8ac8629"
   },
   "outputs": [],
   "source": [
    "selected_features = []\n",
    "remaining_features = list(X_train.columns)\n",
    "best_mse = float('inf')\n",
    "best_features = []\n",
    "\n",
    "while remaining_features:\n",
    "    mse_with_features = {}\n",
    "\n",
    "    for feature in remaining_features:\n",
    "        current_features = selected_features + [feature]\n",
    "\n",
    "        # Scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train[current_features])\n",
    "        X_test_scaled = scaler.transform(X_test[current_features])\n",
    "\n",
    "            # Train model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Evaluate model\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_with_features[feature] = mse\n",
    "\n",
    "        # Find the feature that gives the best improvement\n",
    "    best_feature = min(mse_with_features, key=mse_with_features.get)\n",
    "    best_feature_mse = mse_with_features[best_feature]\n",
    "\n",
    "    if best_feature_mse < best_mse:\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        best_mse = best_feature_mse\n",
    "        best_features = selected_features.copy()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(\"Best Features:\", best_features)\n",
    "print(\"Best MSE:\", best_mse)\n",
    "print(\"size:\", len(best_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-bXKfy82RWJ"
   },
   "source": [
    "So, linear regression works better when we take all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a831yX-N2Isi",
    "outputId": "c490c72b-4217-44b5-b1f6-55aa43a24265"
   },
   "outputs": [],
   "source": [
    "\n",
    "selected_features = []\n",
    "remaining_features = list(X_train.columns)\n",
    "best_mse = float('inf')\n",
    "best_features = []\n",
    "\n",
    "while remaining_features:\n",
    "    mse_with_features = {}\n",
    "\n",
    "    for feature in remaining_features:\n",
    "        current_features = selected_features + [feature]\n",
    "\n",
    "            # Scale data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train[current_features])\n",
    "        X_test_scaled = scaler.transform(X_test[current_features])\n",
    "\n",
    "            # Train XGBoost model\n",
    "        model = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Evaluate model\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_with_features[feature] = mse\n",
    "\n",
    "        # Find the feature that gives the best improvement\n",
    "    best_feature = min(mse_with_features, key=mse_with_features.get)\n",
    "    best_feature_mse = mse_with_features[best_feature]\n",
    "\n",
    "    if best_feature_mse < best_mse:\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        best_mse = best_feature_mse\n",
    "        best_features = selected_features.copy()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Best Features:\", best_features)\n",
    "print(\"Best MSE:\", best_mse)\n",
    "print('size:', len(best_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb7Y0chAKjAE"
   },
   "source": [
    "Let's also check how important are variables that we want to drop out (Distances to SF and SD + Households) for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMac6QCoKiGz",
    "outputId": "6583ede1-7854-469a-d598-b8ffe95efcef"
   },
   "outputs": [],
   "source": [
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "print('R^2 Score:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8YQC6H6L32x"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "QQDHcNcrLkaa",
    "outputId": "5ec9ce85-dae4-4a07-b409-0d01b1065417"
   },
   "outputs": [],
   "source": [
    "importance = model.feature_importances_\n",
    "feature_names = features_list\n",
    "\n",
    "# Create a DataFrame for sorting\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance in XGBoost')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for descending order\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5DUwrNY31Hu"
   },
   "source": [
    "Here we see that with XGBoost the best prediction is coming only from using 7 variables.\n",
    "\n",
    "Most of them are distance variables and neighborhood properties variables are not even in the list.\n",
    "\n",
    "And if we check for feature importance, we get that Distance to SF is indeed important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VquXNoRLFeIj"
   },
   "source": [
    "XGBoost is the best model we have and we want to target to get the best predictions from it.\n",
    "\n",
    "However, there is a trade-off:\n",
    "\n",
    "1) XGBoost prefers to have more distance variables as distance to a city seem to be significant for this model\n",
    "\n",
    "2) Possible problem with linear regressions can arise in case we leave everything as it is\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvOgMdbPvZ7z"
   },
   "source": [
    "Now let's summarize everything:\n",
    "\n",
    "1) Distances are important for XGBoost. XGBoost doesn't care about multicollinearity.\n",
    "\n",
    "2) Distances are multicolinear and it will lead to problems with linear regressions\n",
    "\n",
    "3) XGBoost outperforms Linear models.\n",
    "\n",
    "4) Households are almost not important for the XGBoost but lead to multicolinearity problems\n",
    "\n",
    "Therefore, our final decision is to:\n",
    "\n",
    "1) Leave all distances as they can be important. Cost of it: possible problems with linear regressions with some gains in XGBoost.\n",
    "\n",
    "2) Omit Households variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UXC4t3ZNubc"
   },
   "source": [
    "Additional note:\n",
    "\n",
    "What we also tried to do is to code all distances. The general idea was to code neighborhoods that are in the cities and check the results. This somehow solved the multicolinearity problem\n",
    "\n",
    "However, this led to 2 problems:\n",
    "\n",
    "1) It is very hard to say which neighborhoods are in the city and which are not. Also we need to take agglomearionts into account.\n",
    "\n",
    "2) Many neighborhoods didn't get any class at all: possible data loss.\n",
    "\n",
    "Results were usually lower for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0UIcrKxNTsK"
   },
   "outputs": [],
   "source": [
    "features_list=['Median_Income','Median_Age',\n",
    "               'Tot_Rooms','Tot_Bedrooms','Population',\n",
    "               #'Households',\n",
    "               'Latitude','Longitude',\n",
    "               'Distance_to_coast',\n",
    "               'Distance_to_LA',\n",
    "              'Distance_to_SanJose'\n",
    "               ,'Distance_to_SanDiego','Distance_to_SanFrancisco'\n",
    "               ]\n",
    "\n",
    "\n",
    "y=data['Median_House_Value']\n",
    "X=data[features_list]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Dw5bQZjsBcI"
   },
   "source": [
    "### Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr-zXhRksDXN",
    "outputId": "d197c57b-4b8b-4f3e-c32a-25dffa799be6"
   },
   "outputs": [],
   "source": [
    "LRegression(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsNZSWDpsDen",
    "outputId": "6950bcd8-1872-4fae-ce11-7a0722f2b449"
   },
   "outputs": [],
   "source": [
    "DecisionTreeModel(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYz6mteUsDmN",
    "outputId": "deaca95d-ac89-430d-ea0e-c3e7d640aac8"
   },
   "outputs": [],
   "source": [
    "XGBoost(X_train_scaled,y_train,X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTDCZ8PXO3Uz",
    "outputId": "e8523521-773e-49f4-b435-77d5c7046274"
   },
   "outputs": [],
   "source": [
    "min(2337194506.5773325 , 4633269260.487336 , 4619016653.256039  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLhX4nmXRQs-"
   },
   "source": [
    "Here we see what was expected at the beginning: XGBoost gives us the lowest MSE, while performance of the linear regression is not that accurate. Trees are even worse than Linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u37dNYLHxP8I"
   },
   "source": [
    "## Finetuning and cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxkKOmUzxZeN"
   },
   "source": [
    "First, we split data in training, validation and testing sets.\n",
    "\n",
    "Testing set will be used only at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbX96e8kxHFN"
   },
   "outputs": [],
   "source": [
    "y=data['Median_House_Value']\n",
    "X=data[features_list]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=50)\n",
    "X_val, X_test, y_val , y_test=train_test_split(X_temp, y_temp, test_size=0.5, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPw1O85E8FOm"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcPeUn0kbukC"
   },
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuuFTVGm4P01",
    "outputId": "10dabda3-df05-4f32-ef64-f852e7152d93"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=10,  max_iter=100000)  # alpha is the regularization strength\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lasso = lasso.predict(X_val_scaled)\n",
    "\n",
    "# Evaluation\n",
    "mse_lasso = mean_squared_error(y_val, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_val, y_pred_lasso)\n",
    "print(\"Lasso Regression - MSE:\", mse_lasso, \"R2:\", r2_lasso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnAJyvZz4kbQ",
    "outputId": "914e3c39-6296-44a8-cdb3-1128021db5cd"
   },
   "outputs": [],
   "source": [
    "\n",
    "lasso = Lasso(max_iter=20000)\n",
    "\n",
    "# Define the parameter grid for alpha\n",
    "param_grid = {\n",
    "    \"alpha\": np.logspace(-4, 2, 50)  # Range of alpha values from 0.0001 to 100\n",
    "}\n",
    "\n",
    "# Define a custom scoring metric (e.g., negative mean squared error)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring=scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and corresponding score\n",
    "best_alpha = grid_search.best_params_[\"alpha\"]\n",
    "best_score = -grid_search.best_score_  # Convert back to positive MSE\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_r2= r2_score(y_val,y_val_pred)\n",
    "\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best cross-validated MSE: {best_score}\")\n",
    "print(f\"val MSE: {val_mse}\")\n",
    "print(f\"Validation R^2: {val_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRSOaLRq9S7M"
   },
   "source": [
    "So, best alpha we got for linear Lasso is 0.0001 (which is the lowest possible)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pJgUZ2Ybyn1"
   },
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qvo5I9msmp8",
    "outputId": "bd8d68e6-cece-4844-fad7-e1bb13cae84e"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=1.0)  # alpha controls the regularization strength\n",
    "\n",
    "# Train the model\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_val_pred = ridge.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_test = mean_squared_error(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(f\"Val R²: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibEAJ6YO-u8i"
   },
   "source": [
    "Almost as for Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7exI4A4-uNC",
    "outputId": "7e47f572-3c29-48b8-fd4e-a452a4268465"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "\n",
    "# Define the parameter grid for alpha\n",
    "param_grid = {\n",
    "    \"alpha\": np.logspace(-4, 2, 50)  # Range of alpha values from 0.0001 to 10\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring=\"neg_mean_squared_error\", cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and corresponding score\n",
    "best_alpha = grid_search.best_params_[\"alpha\"]\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "mse_test = mean_squared_error(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(f\"Test R²: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-JcH8ci_Dv4"
   },
   "source": [
    "So the best alpha we got is 0.8286427728546842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJoSfF00byv9"
   },
   "source": [
    "## Polynomial regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHSOcbDA_qj_"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM-LJgOI_lhF"
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly=poly.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5iG6VjM9rFS"
   },
   "source": [
    "Polynomial linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61Wi_KiS_02a",
    "outputId": "c8acb827-a241-4f7e-81db-7c7725ea5a94"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()  # alpha controls the regularization strength\n",
    "\n",
    "lr.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_val_pred = lr.predict(X_val_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_test = mean_squared_error(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(f\"Val R²: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQNcgwRfADeF"
   },
   "source": [
    "Our MSE decreased and R^2 increased which suggests that there is non-linear dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELnb4Neq9oru"
   },
   "source": [
    "Polynomial Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOW28mLrAKAm",
    "outputId": "f4f97b7c-30f6-4b26-8c29-1805ffd06ce3"
   },
   "outputs": [],
   "source": [
    "# Ridge Regression on Polynomial Features\n",
    "ridge_poly = Ridge(alpha=1.0)\n",
    "ridge_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ridge_poly = ridge_poly.predict(X_val_poly)\n",
    "\n",
    "# Evaluation\n",
    "mse_ridge_poly = mean_squared_error(y_val, y_pred_ridge_poly)\n",
    "r2_ridge_poly = r2_score(y_val, y_pred_ridge_poly)\n",
    "print(\"Polynomial Ridge Regression - MSE:\", mse_ridge_poly, \"R2:\", r2_ridge_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez4NEJUsu67M"
   },
   "source": [
    "GridSearch for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmSdvgZiu6Dh",
    "outputId": "47d9692a-e65a-4c09-aa96-5ec53709cb05"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "\n",
    "# Define the parameter grid for alpha\n",
    "param_grid = {\n",
    "    \"alpha\": np.logspace(-4, 2, 50)  # Range of alpha values from 0.0001 to 100\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring=\"neg_mean_squared_error\", cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "\n",
    "# Best parameters and corresponding score\n",
    "best_alpha = grid_search.best_params_[\"alpha\"]\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_val_pred = best_model.predict(X_val_poly)\n",
    "mse_test = mean_squared_error(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(f\"Test R²: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9GVXhvrz-hD"
   },
   "source": [
    "So here we choose smallest possible alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P81pPhzJ9vaQ"
   },
   "source": [
    "Polynomial Lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAeUP19M9oCC",
    "outputId": "01191ef3-f210-4bbd-972f-3514e6df24dc"
   },
   "outputs": [],
   "source": [
    "# Lasso Regression on Polynomial Features\n",
    "lasso_poly = Lasso(alpha=10, max_iter=100000)  # max_iter is increased for convergence\n",
    "lasso_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lasso_poly = lasso_poly.predict(X_val_poly)\n",
    "\n",
    "# Evaluation\n",
    "mse_lasso_poly = mean_squared_error(y_val, y_pred_lasso_poly)\n",
    "r2_lasso_poly = r2_score(y_val, y_pred_lasso_poly)\n",
    "print(\"Polynomial Lasso Regression - MSE:\", mse_lasso_poly, \"R2:\", r2_lasso_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXtEFJbvw0T2"
   },
   "source": [
    "It took more than 2 minutes to execute the code with the previous cell.\n",
    "\n",
    "Possible problem: GridSearch for polynomial Lasso will take much more time. Or there can be a problem with the gradient descent convergence.\n",
    "\n",
    "Now we do gridsearch for optimal alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqpAFMNhwxy2",
    "outputId": "557577ef-6493-4261-d5da-2688f40e84e4"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter=20000)\n",
    "\n",
    "# Define the parameter grid for alpha\n",
    "param_grid = {\n",
    "    \"alpha\": np.logspace(-4, 2, 15)  # Alpha values ranging from 10^-4 to 10^2\n",
    "}\n",
    "\n",
    "# Define a scoring function (negative MSE for minimization)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring=scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "\n",
    "# Best parameters and corresponding score\n",
    "best_alpha = grid_search.best_params_[\"alpha\"]\n",
    "best_score = -grid_search.best_score_  # Convert back to positive MSE\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val_poly)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_r2= r2_score(y_val,y_val_pred)\n",
    "\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best cross-validated MSE: {best_score}\")\n",
    "print(f\"val MSE: {val_mse}\")\n",
    "print(f\"Validation R^2: {val_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdOY1XHfznsc"
   },
   "source": [
    "It is not increasing that much with a change of alpha\n",
    "\n",
    "At the same time, it took more than 25 minutes to complete it and it still did not converge.\n",
    "\n",
    "That is a problem due to multicollinearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG2VtR0aCF-u"
   },
   "source": [
    "What we get from this: polynomial method is improving our predictions, so in this dataset there is some non-linear relations. Now we will check it with degree of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVJwBJcFCEU8",
    "outputId": "bf16b879-2c82-4079-eeb3-ff7c73241235"
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly=poly.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "lr = LinearRegression()  # alpha controls the regularization strength\n",
    "\n",
    "lr.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_val_pred = lr.predict(X_val_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_test = mean_squared_error(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(f\"Val R²: {r2_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhFKRfQlCgoj"
   },
   "source": [
    "Improvement in predictive power is not that strong, so we will leave degree=2 for the future.\n",
    "\n",
    "Also increasing degrees further will be very time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5eL4XNZCf64"
   },
   "outputs": [],
   "source": [
    "#for future\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly=poly.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T40jyP8sbyyJ"
   },
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25Th8yeJDHlb",
    "outputId": "0bdc93a2-1dfd-4d00-dede-28eb468d547b"
   },
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(alpha=1, l1_ratio=0.5)\n",
    "# alpha controls the overall regularization strength\n",
    "# l1_ratio controls the mix of Lasso (L1) and Ridge (L2) penalties (0.5 means equal mix)\n",
    "\n",
    "# Train the model\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_val_pred = elastic_net.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_test = mean_squared_error(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Valdiation MSE: {mse_test}\")\n",
    "print(f\"Validation R²: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2798vCSGaOuK"
   },
   "source": [
    "Strangely, $R^2$ is much lower here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxGw2NH_Dsxz",
    "outputId": "f2678b04-e92a-4d0a-b0a1-67734b9723ef"
   },
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(alpha=1, l1_ratio=0.5)\n",
    "elastic_net.fit(X_train_poly, y_train)\n",
    "y_val_pred = elastic_net.predict(X_val_poly)\n",
    "mse_test = mean_squared_error(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Valdiation MSE: {mse_test}\")\n",
    "print(f\"Validation R²: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K36RgtIaD_g9"
   },
   "source": [
    "Accuracy for some reason fell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4i5ANBs226A"
   },
   "source": [
    "We see here that Elastic Net is not a really regularization method for this data. Therefore, there is even no reason for starting GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zDSxF_R4ah4"
   },
   "source": [
    "Now let's check for Bias-Variance decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZtPw92_xc_dA",
    "outputId": "478ec2fa-33ed-4575-d81e-e093f01bcc72"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Parameters\n",
    "n_iterations = 50  # Number of bootstrap samples for approximation\n",
    "test_predictions_ridge = []\n",
    "test_predictions_lasso = []\n",
    "test_predictions_ridge_poly = []\n",
    "test_predictions_lasso_poly = []\n",
    "test_predictions_basic = []\n",
    "test_predictions_basic_poly = []\n",
    "\n",
    "# Fit PolynomialFeatures only on training data\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)  # Example: Degree 2 for polynomial features\n",
    "poly.fit(X_train_scaled)  # Fit only once\n",
    "X_train_poly = poly.transform(X_train_scaled)\n",
    "X_val_scaled_poly = poly.transform(X_val_scaled)\n",
    "\n",
    "# Perform bias-variance decomposition for each model\n",
    "for i in range(n_iterations):\n",
    "    # Bootstrap sample from the training set\n",
    "    X_train_sample, y_train_sample = resample(X_train_scaled, y_train)\n",
    "\n",
    "    # Linear Regression - Basic\n",
    "    BasicRegression = LinearRegression()\n",
    "    BasicRegression.fit(X_train_sample, y_train_sample)\n",
    "    test_predictions_basic.append(BasicRegression.predict(X_val_scaled))\n",
    "\n",
    "    # Linear Regression - Polynomial\n",
    "    X_train_sample_poly = poly.transform(X_train_sample)  # Transform bootstrap sample\n",
    "    BasicPoly = LinearRegression()\n",
    "    BasicPoly.fit(X_train_sample_poly, y_train_sample)\n",
    "    test_predictions_basic_poly.append(BasicPoly.predict(X_val_scaled_poly))\n",
    "\n",
    "    # Ridge Regression - Basic\n",
    "    ridge = Ridge(alpha=0.9)\n",
    "    ridge.fit(X_train_sample, y_train_sample)\n",
    "    test_predictions_ridge.append(ridge.predict(X_val_scaled))\n",
    "\n",
    "    # Lasso Regression - Basic\n",
    "    lasso = Lasso(alpha=8)\n",
    "    lasso.fit(X_train_sample, y_train_sample)\n",
    "    test_predictions_lasso.append(lasso.predict(X_val_scaled))\n",
    "\n",
    "    # Ridge Regression - Polynomial\n",
    "    ridge_poly = Ridge(alpha=0.01)\n",
    "    ridge_poly.fit(X_train_sample_poly, y_train_sample)\n",
    "    test_predictions_ridge_poly.append(ridge_poly.predict(X_val_scaled_poly))\n",
    "\n",
    "    # Lasso Regression - Polynomial\n",
    "    lasso_poly = Lasso(alpha=0.78, max_iter=10000)\n",
    "    lasso_poly.fit(X_train_sample_poly, y_train_sample)\n",
    "    test_predictions_lasso_poly.append(lasso_poly.predict(X_val_scaled_poly))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "test_predictions_basic = np.array(test_predictions_basic)\n",
    "test_predictions_basic_poly = np.array(test_predictions_basic_poly)\n",
    "test_predictions_ridge = np.array(test_predictions_ridge)\n",
    "test_predictions_lasso = np.array(test_predictions_lasso)\n",
    "test_predictions_ridge_poly = np.array(test_predictions_ridge_poly)\n",
    "test_predictions_lasso_poly = np.array(test_predictions_lasso_poly)\n",
    "\n",
    "# Bias-Variance decomposition function\n",
    "def bias_variance(y_test, predictions):\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    bias = np.mean((avg_predictions - y_test) ** 2)\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    return bias, variance\n",
    "\n",
    "# Bias-Variance decomposition for each model\n",
    "bias_basic, variance_basic = bias_variance(y_val, test_predictions_basic)\n",
    "bias_basic_poly, variance_basic_poly = bias_variance(y_val, test_predictions_basic_poly)\n",
    "bias_ridge, variance_ridge = bias_variance(y_val, test_predictions_ridge)\n",
    "bias_lasso, variance_lasso = bias_variance(y_val, test_predictions_lasso)\n",
    "bias_ridge_poly, variance_ridge_poly = bias_variance(y_val, test_predictions_ridge_poly)\n",
    "bias_lasso_poly, variance_lasso_poly = bias_variance(y_val, test_predictions_lasso_poly)\n",
    "\n",
    "# Plot Bias and Variance\n",
    "models = ['Linear Regression', 'Polynomial Linear Regression', 'Ridge Linear Regression',\n",
    "          'Lasso Linear Regression', 'Ridge Polynomial Regression', 'Lasso Polynomial Regression']\n",
    "biases = [bias_basic, bias_basic_poly, bias_ridge, bias_lasso, bias_ridge_poly, bias_lasso_poly]\n",
    "variances = [variance_basic, variance_basic_poly, variance_ridge, variance_lasso, variance_ridge_poly, variance_lasso_poly]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(models))\n",
    "plt.bar(x - 0.2, biases, 0.4, label='Bias^2')\n",
    "plt.bar(x + 0.2, variances, 0.4, label='Variance')\n",
    "plt.xticks(x, models, rotation=15)\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Bias-Variance Decomposition\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Graphg is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHyNeoJSgEHK"
   },
   "source": [
    "Our model is underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GniiVDRHlTr-"
   },
   "source": [
    "In this HW we worked a lot with linear models. It became obvious after VIF tests that there is multicollinearity in this data.\n",
    "\n",
    " We tried to do many things:\n",
    "\n",
    "1) Omitting variables (which helped us somehow)\n",
    "\n",
    "2) Implementing variable selection (which gave us that using all variables is the best option).\n",
    "\n",
    "3) Trying to encode distances for city agglomeration they are located in.\n",
    "\n",
    "4) Trying different versions of linear models (Lasso, Ridge, Elastic Net).\n",
    "\n",
    "5) Trying polynomial features, which gave us significantly higher $R^2$\n",
    "\n",
    "6) Trying bias-variance decomposition\n",
    "\n",
    "But all the time we were getting odd results.\n",
    "\n",
    "1) Coefficents for Ridge, Elastic Net and Lasso were not making sence\n",
    "\n",
    "2) Gradient descent for Lasso regression was converging only after setting 100K as maximum iterations (which is very costly in terms of computational power).\n",
    "\n",
    "3) Polynomial features gave better MSE.\n",
    "\n",
    "4) Bias-Variance decomposition indicated that model is underfitting.\n",
    "the model is underfitting.\n",
    "\n",
    "This all gives us an understanding that using Linear Models for this dataset is not a good decision since relationships between X and Y are not linear. Therefore, it would be better to use other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbmcqOEMby0Y"
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ouWnu3Dj9XR",
    "outputId": "ed094a42-41e6-44b9-b33e-326caf67a01f"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=42, max_depth=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "br0np2ITzMby",
    "outputId": "7bb364da-6865-44d7-c468-9a5c3c61dd58"
   },
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "jOjyr82tqvpT",
    "outputId": "0789f4ce-590c-41af-cb7c-77e0e3c4a309"
   },
   "outputs": [],
   "source": [
    "feature_importance = model.feature_importances_\n",
    "\n",
    "    # Plot Feature Importance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(features_list, feature_importance, color='skyblue')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance in Decision Tree Regressor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpnATQp-0o2v",
    "outputId": "74edc8e0-7d83-4d32-bb00-47875902263a"
   },
   "outputs": [],
   "source": [
    "#Grid Search for Decission Trees\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit to training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNJiublW1LkJ",
    "outputId": "ff82c7c9-a293-48b2-a3e7-e226a3e5f1ec"
   },
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8G_4KTC1pKn",
    "outputId": "953ed7d5-6e47-4aa7-9ad3-0f71ea3faaca"
   },
   "outputs": [],
   "source": [
    "3696204082.82/(10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "sAQdn6gV1SW7",
    "outputId": "85c64113-8ba9-4e47-c61f-8d1a2fd3fc5a"
   },
   "outputs": [],
   "source": [
    "feature_importance = best_model.feature_importances_\n",
    "\n",
    "    # Plot Feature Importance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(features_list, feature_importance, color='skyblue')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance in Decision Tree Regressor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hJ3bG151Yb1"
   },
   "source": [
    "What can we say here?\n",
    "\n",
    "1) We see that validation MSE is $3.69*10^9$\n",
    "\n",
    "2) The most important features are distance to coast and Median Income. Other independent variables are not that inportant.\n",
    "\n",
    "3) Interesting thing here that location and neighborhood income (or prestige therefore) is more important than housing parameters such as number of bedrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYvkfkkCb-Co"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KUOc_M-bx-S",
    "outputId": "5ac28520-4d51-40b2-feef-3e95993e1097"
   },
   "outputs": [],
   "source": [
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "print('Mean Squared Error:', mse)\n",
    "print('R^2 Score:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1YOnKPj2jOy",
    "outputId": "444fb9a5-9397-4d90-b701-f6df93aa2192"
   },
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],         # Number of trees\n",
    "    'max_depth': [3, 5, 7],                # Tree depth\n",
    "    'learning_rate': [0.01, 0.1, 0.2],     # Step size shrinkage\n",
    "    'min_child_weight': [1, 3, 5],         # Minimum child weight\n",
    "    'gamma': [0, 0.1, 0.2],                # Minimum loss reduction\n",
    "    'subsample': [0.8, 1.0],               # Fraction of samples per tree\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = best_model.predict(X_val_scaled)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R^2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "-i314m2F2jTW",
    "outputId": "4bdbadee-d6e7-4c8e-bf7b-1adf4f9454ea"
   },
   "outputs": [],
   "source": [
    "importance =best_model.feature_importances_\n",
    "feature_names = features_list\n",
    "\n",
    "# Create a DataFrame for sorting\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance in XGBoost')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for descending order\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppf9-rvYltHl"
   },
   "source": [
    "Here we see that the most important features are Median income, Distance to coast and Distances to cities.\n",
    "\n",
    "At the same time, for this model, neighborhood properties are not that important.\n",
    "\n",
    "So it means, that for XGBoost location of the neighborhood and it's is the most important information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98jeEn573OQ9"
   },
   "source": [
    "## Final model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrisAKWpuBb9"
   },
   "source": [
    "Here now we are using our Test dataset.\n",
    "\n",
    "We will use the best models we got from fine-tuning (i.e best variations with best hyperparameters) in the previous part and obtain new metric.\n",
    "\n",
    "After that we can comment on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gbL_Tj-euAuZ",
    "outputId": "86396bb1-9fbd-4424-bfbb-95779345cda8"
   },
   "outputs": [],
   "source": [
    "#Linear model:\n",
    "\n",
    "model=LinearRegression()\n",
    "\n",
    "model.fit(X_train_poly, y_train)\n",
    "y_pred = model.predict(X_test_poly)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Model MSE is {mse} and its R^2 is: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gR9rgDH5uY8I",
    "outputId": "9c21f72c-6c76-4034-bd94-ec0dcef593a8"
   },
   "outputs": [],
   "source": [
    "#Decision Trees:\n",
    "model = DecisionTreeRegressor(\n",
    "    max_depth=None,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Model MSE is {mse} and its R^2 is: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSfWmQSuMEyR",
    "outputId": "1bd3617e-fee1-465b-eff4-6d190b04347f"
   },
   "outputs": [],
   "source": [
    "#XGBoost:\n",
    "\n",
    "model=XGBRegressor(gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    min_child_weight=1,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    objective='reg:squarederror',  # Objective for regression\n",
    "    random_state=42)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred=model.predict(X_test_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae=mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f'Model MSE is {mse} and its R^2 is: {r2}')\n",
    "print(f'Model MAE is {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9BoY-5HMFob"
   },
   "source": [
    "This results are leading to such conclusions:\n",
    "\n",
    "1) As expected, XGBoost show really good results\n",
    "\n",
    "2) Linear model with polynomial features is also quite accurate and is as accurate as Decision Trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJm-uhussSWm"
   },
   "source": [
    "Our final selection is biased as at the beginning it was told that XGBoost is the best performing model.\n",
    "\n",
    "After fine-tuning all 3 models on the testing set we still get that XGBoost has the lowest MSE.\n",
    "\n",
    "Possible way to improve models: to go deeper into models, choose more parameters and fine a way to lower MSE. However, after more than 100 runs of different models on different subsets of data with different parameters it seems that improvements will not lead to a much better performance. So the only way is to wait for a completely new models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvDHKzF6t8b1"
   },
   "source": [
    "The most interesting insight we got is that when trying to predict the median house value of a neighborhood (so an average price of a house somewhere) is to look at the location and neighbors. And on the contrary, population / comfort level of houses (number of rooms or the age of the houses) are not that important.\n",
    "\n",
    "And that is great indeed because it really supports the common view that in the US housing market is more dependent on the people who leave nearby and how prestigious is the neighborhood, rather than anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSJgVgS1vqH_"
   },
   "source": [
    "At the end, our best model can make predictions with an $R^2$ of 0.84 which is somewhat accurate.\n",
    "\n",
    "What is significant here is that MAE of our model on the test set is less than 30K$. So on average, our algorithm make mistake of 30 thousand US dollars.\n",
    "\n",
    "However, mean price in this dataset is 206K4 , while it's standard deviation is 115k$. So our predictions there are somehow accurate and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30XRfiFBymyl"
   },
   "source": [
    "So, using this model we can somehow accurately estimate a median housing price in any neighborhood in California and that is exactly what we wanted to do initially.\n",
    "\n",
    "This model can be used by real estate agencies in the state for housing pricing process or for policymakers of the state of California for finding solutions to the current housing crisis.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
